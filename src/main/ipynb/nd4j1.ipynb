{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# nd4j1\n",
    "\n",
    "This workbook is intended to provide a the \"MVP\" of a jupyter enabled notebook for [deeplearning4j](https://deeplearning4j.org). Its goal is to provide a working example of a DL4J network, and then to use [wisp](https://github.com/quantifind/wisp) to display the results of the network classification output.\n",
    "\n",
    "This example is based on the classic classicication problem using [iris flower data](https://en.wikipedia.org/wiki/Iris_flower_data_set). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "classpath.addPath(\"/opt/app/libs/datavec-api-0.5.0.jar\")\n",
    "classpath.addPath(\"/opt/app/libs/annotations-2.0.1.jar\")                \n",
    "classpath.addPath(\"/opt/app/libs/commons-io-2.4.jar\")\n",
    "classpath.addPath(\"/opt/app/libs/nd4j-native-0.5.0-linux-x86_64.jar\")\n",
    "classpath.addPath(\"/opt/app/libs/commons-lang3-3.3.1.jar\")\n",
    "classpath.addPath(\"/opt/app/libs/commons-math3-3.4.1.jar\")\n",
    "classpath.addPath(\"/opt/app/libs/guava-18.0.jar\")\n",
    "classpath.addPath(\"/opt/app/libs/nd4j-native-0.5.0.jar\")\n",
    "classpath.addPath(\"/opt/app/libs/javacpp-1.2.3.jar\")\n",
    "classpath.addPath(\"/opt/app/libs/nd4j-native-api-0.5.0.jar\")\n",
    "classpath.addPath(\"/opt/app/libs/javassist-3.18.2-GA.jar\")\n",
    "classpath.addPath(\"/opt/app/libs/nd4j-native-platform-0.5.0.jar\")\n",
    "classpath.addPath(\"/opt/app/libs/lombok-1.16.4.jar\")\n",
    "classpath.addPath(\"/opt/app/libs/reflections-0.9.10.jar\")\n",
    "classpath.addPath(\"/opt/app/libs/nd4j-api-0.5.0.jar\")\n",
    "classpath.addPath(\"/opt/app/libs/scala-library-2.11.1.jar\")\n",
    "classpath.addPath(\"/opt/app/libs/nd4j-buffer-0.5.0.jar\")\n",
    "classpath.addPath(\"/opt/app/libs/slf4j-api-1.7.21.jar\")\n",
    "classpath.addPath(\"/opt/app/libs/nd4j-common-0.5.0.jar\")\n",
    "classpath.addPath(\"/opt/app/libs/slf4j-simple-1.7.21.jar\")\n",
    "classpath.addPath(\"/opt/app/libs/nd4j-context-0.5.0.jar\")\n",
    "classpath.addPath(\"/opt/app/libs/deeplearning4j-core-0.5.0.jar\")\n",
    "classpath.addPath(\"/opt/app/libs/canova-api-0.0.0.17.jar\")\n",
    "classpath.addPath(\"/opt/app/libs/hadoop-mapreduce-client-core-2.2.0.jar\")\n",
    "classpath.addPath(\"/opt/app/libs/datavec-nd4j-common-0.5.0.jar\")\n",
    "classpath.addPath(\"/opt/app/libs/reflections-0.9.10.jar\")\n",
    "classpath.addPath(\"/opt/app/libs/jackson-dataformat-yaml-2.4.4.jar\")\n",
    "classpath.addPath(\"/opt/app/libs/guava-18.0.jar\")\n",
    "classpath.addPath(\"/opt/app/libs/jackson-core-2.6.5.jar\")\n",
    "classpath.addPath(\"/opt/app/libs/jackson-annotations-2.6.5.jar\")\n",
    "classpath.addPath(\"/opt/app/libs/jackson-databind-2.6.5.jar\")\n",
    "classpath.addPath(\"/opt/app/libs/jackson-module-scala_2.10-2.6.5.jar\")\n",
    "classpath.addPath(\"/opt/app/libs/json4s-jackson_2.10-3.2.11.jar\")\n",
    "classpath.addPath(\"/opt/app/libs/commons-lang-2.5.jar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding 27 artifact(s)\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "classpath.add(\"com.quantifind\" %% \"wisp\" % \"0.0.4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[36mjava.text.{DecimalFormat, DecimalFormatSymbols}\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.commons.lang.StringUtils\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.nd4j.linalg.api.complex.IComplexNDArray\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.nd4j.linalg.api.ndarray.INDArray\u001b[0m\n",
       "defined \u001b[32mclass \u001b[36mNDArrayStrings\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import java.text.{DecimalFormat, DecimalFormatSymbols}\n",
    "\n",
    "import org.apache.commons.lang.StringUtils\n",
    "import org.nd4j.linalg.api.complex.IComplexNDArray\n",
    "import org.nd4j.linalg.api.ndarray.INDArray\n",
    "\n",
    "/**\n",
    "  * Created by claytongraham on 10/29/16.\n",
    "  *\n",
    "  * we have to do this because the dl4j implementation does not let the user change the format, this\n",
    "  * can be an issue when you want to use the serialization model for other things like making a \n",
    "  * guava table from the string representation. What would be really awesome is if someone were \n",
    "  * smart enough to provide better data ingress and egress for nd4j. If this gets released then \n",
    "  * remove this and use nd4j\n",
    "  * \n",
    "  * https://github.com/deeplearning4j/nd4j/blob/master/nd4j-backends/nd4j-api-parent/nd4j-api/src/main/java/org/nd4j/linalg/string/NDArrayStrings.java\n",
    "  */\n",
    "class NDArrayStrings {\n",
    "  private var sep: String = \",\"\n",
    "  private var padding: Int = 0\n",
    "  private var decFormatNum: String = \"#,###,##0\"\n",
    "  private var decFormatRest: String = \"\"\n",
    "  private var decimalFormat: DecimalFormat = new DecimalFormat(decFormatNum + decFormatRest)\n",
    "\n",
    "  def this(sep: String, precisionI: Int, decFormat: String) {\n",
    "    this()\n",
    "    this.decFormatNum = decFormat\n",
    "    this.sep = sep\n",
    "    var precision: Int = precisionI\n",
    "\n",
    "    if (precision != 0) {\n",
    "      this.decFormatRest = \".\"\n",
    "      while (precision > 0) {\n",
    "        this.decFormatRest += \"0\"\n",
    "        precision -= 1\n",
    "      }\n",
    "    }\n",
    "    this.decimalFormat = new DecimalFormat(decFormatNum + decFormatRest)\n",
    "    val sepNgroup: DecimalFormatSymbols = DecimalFormatSymbols.getInstance\n",
    "    sepNgroup.setDecimalSeparator('.')\n",
    "    sepNgroup.setGroupingSeparator(',')\n",
    "    decimalFormat.setDecimalFormatSymbols(sepNgroup)\n",
    "  }\n",
    "\n",
    "  /**\n",
    "    * Format the given ndarray as a string\n",
    "    *\n",
    "    * @param arr the array to format\n",
    "    * @return the formatted array\n",
    "    */\n",
    "  def format(arr: INDArray): String = {\n",
    "    val padding: String = decimalFormat.format(arr.maxNumber)\n",
    "    this.padding = padding.length\n",
    "    format(arr, arr.rank)\n",
    "  }\n",
    "\n",
    "  private def format(arr: INDArray, rank: Int): String = format(arr, arr.rank, 0)\n",
    "\n",
    "  private def format(arr: INDArray, rank: Int, offsetI: Int): String = {\n",
    "    val sb: StringBuilder = new StringBuilder\n",
    "    var offset: Int = offsetI\n",
    "    if (arr.isScalar) {\n",
    "      if (arr.isInstanceOf[IComplexNDArray]) return arr.asInstanceOf[IComplexNDArray].getComplex(0).toString\n",
    "      decimalFormat.format(arr.getDouble(0))\n",
    "    }\n",
    "    else if (rank <= 0) \"\"\n",
    "    else if (arr.isVector) {\n",
    "      sb.append(\"[\")\n",
    "      var i: Int = 0\n",
    "      while (i < arr.length) {\n",
    "        {\n",
    "          if (arr.isInstanceOf[IComplexNDArray]) sb.append(arr.asInstanceOf[IComplexNDArray].getComplex(i).toString)\n",
    "          else sb.append(String.format(\"%1$\" + padding + \"s\", decimalFormat.format(arr.getDouble(i))))\n",
    "          if (i < arr.length - 1) sb.append(sep)\n",
    "        }\n",
    "        {\n",
    "          i += 1; i - 1\n",
    "        }\n",
    "      }\n",
    "      sb.append(\"]\")\n",
    "      sb.toString\n",
    "    }\n",
    "    else {\n",
    "      offset = offset + 1\n",
    "      sb.append(\"[\")\n",
    "      var i: Int = 0\n",
    "      while (i < arr.slices) {\n",
    "        {\n",
    "          sb.append(format(arr.slice(i), rank - 1, offset))\n",
    "          if (i != arr.slices - 1) {\n",
    "            sb.append(\",\\n\")\n",
    "            sb.append(StringUtils.repeat(\"\\n\", rank - 2))\n",
    "            sb.append(StringUtils.repeat(\" \", offset))\n",
    "          }\n",
    "        }\n",
    "        {\n",
    "          i += 1; i - 1\n",
    "        }\n",
    "      }\n",
    "      sb.append(\"]\")\n",
    "      sb.toString\n",
    "    }\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[36mjava.io.{File, IOException}\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.commons.io.FileUtils\u001b[0m\n",
       "\u001b[32mimport \u001b[36mjava.net.URL\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.nd4j.linalg.factory.Nd4j\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.datavec.api.records.reader.RecordReader\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.datavec.api.records.reader.impl.csv.CSVRecordReader\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.datavec.api.split.FileSplit\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.datavec.api.util.ClassPathResource\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.deeplearning4j.datasets.datavec.RecordReaderDataSetIterator\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.deeplearning4j.eval.Evaluation\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.deeplearning4j.nn.conf.MultiLayerConfiguration\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.deeplearning4j.nn.conf.NeuralNetConfiguration\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.deeplearning4j.nn.conf.layers.DenseLayer\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.deeplearning4j.nn.conf.layers.OutputLayer\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.deeplearning4j.nn.multilayer.MultiLayerNetwork\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.deeplearning4j.nn.weights.WeightInit\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.deeplearning4j.optimize.listeners.ScoreIterationListener\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.nd4j.linalg.api.ndarray.INDArray\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.nd4j.linalg.dataset.DataSet\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.nd4j.linalg.dataset.SplitTestAndTrain\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.nd4j.linalg.dataset.api.iterator.DataSetIterator\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.nd4j.linalg.dataset.api.preprocessor.DataNormalization\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.nd4j.linalg.dataset.api.preprocessor.NormalizerStandardize\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.nd4j.linalg.lossfunctions.LossFunctions\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.slf4j.Logger\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.slf4j.LoggerFactory\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.reflections.Reflections\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.reflections.scanners.SubTypesScanner\u001b[0m\n",
       "\u001b[32mimport \u001b[36mjava.util\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.datavec.common.data.NDArrayWritable\u001b[0m\n",
       "\u001b[32mimport \u001b[36mcom.fasterxml.jackson.core.JsonParseException\u001b[0m\n",
       "\u001b[32mimport \u001b[36mcom.fasterxml.jackson.databind.ObjectMapper\u001b[0m\n",
       "\u001b[32mimport \u001b[36mcom.google.common.collect.{HashBasedTable, Table}\u001b[0m\n",
       "defined \u001b[32mclass \u001b[36mDeepLearning4JMultiLayerNetwork\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import java.io.{File, IOException}\n",
    "import org.apache.commons.io.FileUtils\n",
    "import java.net.URL\n",
    "import org.nd4j.linalg.factory.Nd4j\n",
    "import org.datavec.api.records.reader.RecordReader\n",
    "import org.datavec.api.records.reader.impl.csv.CSVRecordReader\n",
    "import org.datavec.api.split.FileSplit\n",
    "import org.datavec.api.util.ClassPathResource\n",
    "import org.deeplearning4j.datasets.datavec.RecordReaderDataSetIterator\n",
    "import org.deeplearning4j.eval.Evaluation\n",
    "import org.deeplearning4j.nn.conf.MultiLayerConfiguration\n",
    "import org.deeplearning4j.nn.conf.NeuralNetConfiguration\n",
    "import org.deeplearning4j.nn.conf.layers.DenseLayer\n",
    "import org.deeplearning4j.nn.conf.layers.OutputLayer\n",
    "import org.deeplearning4j.nn.multilayer.MultiLayerNetwork\n",
    "import org.deeplearning4j.nn.weights.WeightInit\n",
    "import org.deeplearning4j.optimize.listeners.ScoreIterationListener\n",
    "import org.nd4j.linalg.api.ndarray.INDArray\n",
    "import org.nd4j.linalg.dataset.DataSet\n",
    "import org.nd4j.linalg.dataset.SplitTestAndTrain\n",
    "import org.nd4j.linalg.dataset.api.iterator.DataSetIterator\n",
    "import org.nd4j.linalg.dataset.api.preprocessor.DataNormalization\n",
    "import org.nd4j.linalg.dataset.api.preprocessor.NormalizerStandardize\n",
    "import org.nd4j.linalg.lossfunctions.LossFunctions\n",
    "import org.slf4j.Logger\n",
    "import org.slf4j.LoggerFactory\n",
    "import org.reflections.Reflections\n",
    "import org.reflections.scanners.SubTypesScanner\n",
    "import java.util\n",
    "import org.datavec.common.data.NDArrayWritable\n",
    "import com.fasterxml.jackson.core.JsonParseException\n",
    "import com.fasterxml.jackson.databind.ObjectMapper\n",
    "import com.google.common.collect.{HashBasedTable, Table}\n",
    "\n",
    "/**\n",
    "  * Created by claytongraham on 10/29/16.\n",
    "  *\n",
    "  * this is our network executor, it is a classifier that tries to figure out\n",
    "  * what type of iris we have for each based on the 5 attributes\n",
    "  * \n",
    "  */\n",
    "class DeepLearning4JMultiLayerNetwork(val filePath: String) {\n",
    "\n",
    "  var dataFile: String = filePath\n",
    "\n",
    "  var networkOutput: Table[Integer, Integer, Double] = null\n",
    "  \n",
    "  def execute() {\n",
    "    println(\"Hello, deeplearning4j!\")\n",
    "\n",
    "    //First: get the dataset using the record reader. CSVRecordReader handles loading/parsing\n",
    "    val numLinesToSkip: Int = 0\n",
    "    val delimiter: String = \",\"\n",
    "    val recordReader: RecordReader = new CSVRecordReader(numLinesToSkip, delimiter)\n",
    "    recordReader.initialize(new FileSplit(new File(dataFile)))\n",
    "\n",
    "    //Second: the RecordReaderDataSetIterator handles conversion to DataSet objects, ready for use in neural network\n",
    "    val labelIndex: Int = 4 //5 values in each row of the iris.txt CSV: 4 input features followed by an integer label (class) index. Labels are the 5th value (index 4) in each row\n",
    "    val numClasses: Int = 3 //3 classes (types of iris flowers) in the iris data set. Classes have integer values 0, 1 or 2\n",
    "    val batchSize: Int = 150 //Iris data set: 150 examples total. We are loading all of them into one DataSet (not recommended for large data sets)\n",
    "\n",
    "    val iterator: DataSetIterator = new RecordReaderDataSetIterator(recordReader, batchSize, labelIndex, numClasses)\n",
    "\n",
    "    val allData: DataSet = iterator.next\n",
    "    allData.shuffle()\n",
    "    val testAndTrain: SplitTestAndTrain = allData.splitTestAndTrain(0.65) //Use 65% of data for training\n",
    "    val trainingData: DataSet = testAndTrain.getTrain\n",
    "    val testData: DataSet = testAndTrain.getTest\n",
    "\n",
    "    //We need to normalize our data. We'll use NormalizeStandardize (which gives us mean 0, unit variance):\n",
    "    val normalizer: DataNormalization = new NormalizerStandardize\n",
    "    normalizer.fit(trainingData) //Collect the statistics (mean/stdev) from the training data. This does not modify the input data\n",
    "    normalizer.transform(trainingData) //Apply normalization to the training data\n",
    "    normalizer.transform(testData) //Apply normalization to the test data. This is using statistics calculated from the *training* set\n",
    "    val numInputs: Int = 4\n",
    "    val outputNum: Int = 3\n",
    "    val iterations: Int = 1000\n",
    "    val seed: Long = 6\n",
    "\n",
    "    println(\"Build model....\")\n",
    "    val conf: MultiLayerConfiguration =\n",
    "      new NeuralNetConfiguration.Builder().seed(seed)\n",
    "        .iterations(iterations).activation(\"tanh\")\n",
    "        .weightInit(WeightInit.XAVIER).learningRate(0.1)\n",
    "        .regularization(true).l2(1e-4).list\n",
    "        .layer(0, new DenseLayer.Builder().nIn(numInputs).nOut(3).build)\n",
    "        .layer(1, new DenseLayer.Builder().nIn(3).nOut(3).build)\n",
    "        .layer(2, new OutputLayer.Builder(LossFunctions.LossFunction.NEGATIVELOGLIKELIHOOD)\n",
    "          .activation(\"softmax\")\n",
    "          .nIn(3).nOut(outputNum).build).backprop(true).pretrain(false).build\n",
    "\n",
    "    //run the model\n",
    "    val model: MultiLayerNetwork = new MultiLayerNetwork(conf)\n",
    "    model.init()\n",
    "    model.setListeners(new ScoreIterationListener(100))\n",
    "    model.fit(trainingData)\n",
    "\n",
    "    //evaluate the model on the test set\n",
    "    val eval: Evaluation = new Evaluation(3)\n",
    "    val output: INDArray = model.output(testData.getFeatureMatrix)\n",
    "    eval.eval(testData.getLabels, output)\n",
    "    println(eval.stats)\n",
    "    \n",
    "    networkOutput = makeTableFromArray(output,3)\n",
    "    \n",
    "  }\n",
    "\n",
    "\n",
    "  @throws[IOException]\n",
    "  def makeRowsFromNDArray(source: INDArray, precision: Int): util.List[_] = {\n",
    "    val mapper: ObjectMapper = new ObjectMapper\n",
    "    val serializedData: String = new NDArrayStrings(\",\", precision, \"######0\").format(source)\n",
    "    try {\n",
    "      val rows: util.List[_] = mapper.readValue(serializedData.getBytes, classOf[util.List[_]]).asInstanceOf[util.List[_]]\n",
    "      rows\n",
    "    }\n",
    "    catch {\n",
    "      case e: JsonParseException => {\n",
    "        e.printStackTrace()\n",
    "        return null\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "\n",
    "  @throws[IOException]\n",
    "  def makeTableFromArray(source: INDArray, precision: Int): Table[Integer, Integer, Double] = {\n",
    "    val table: Table[Integer, Integer, Double] = HashBasedTable.create[Integer, Integer, Double]\n",
    "    val rows: util.List[_] = makeRowsFromNDArray(source, precision)\n",
    "    var i: Int = 0\n",
    "    while (i < rows.size) {\n",
    "      {\n",
    "        val row: util.List[Double] = rows.get(i).asInstanceOf[util.List[Double]]\n",
    "        var j: Int = 0\n",
    "        while (j < row.size) {\n",
    "          {\n",
    "            table.put(i, j, row.get(j))\n",
    "          }\n",
    "          {\n",
    "            j += 1; j - 1\n",
    "          }\n",
    "        }\n",
    "      }\n",
    "      {\n",
    "        i += 1; i - 1\n",
    "      }\n",
    "    }\n",
    "    table\n",
    "  }\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, deeplearning4j!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[pool-4-thread-10] INFO org.reflections.Reflections - Reflections took 466 ms to scan 7 urls, producing 109 keys and 364 values \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[pool-4-thread-10] INFO org.deeplearning4j.optimize.listeners.ScoreIterationListener - Score at iteration 0 is 1.0732334011203644\n",
      "[pool-4-thread-10] INFO org.deeplearning4j.optimize.listeners.ScoreIterationListener - Score at iteration 100 is 0.4338571233450501\n",
      "[pool-4-thread-10] INFO org.deeplearning4j.optimize.listeners.ScoreIterationListener - Score at iteration 200 is 0.38194244254622567\n",
      "[pool-4-thread-10] INFO org.deeplearning4j.optimize.listeners.ScoreIterationListener - Score at iteration 300 is 0.20852557196019797\n",
      "[pool-4-thread-10] INFO org.deeplearning4j.optimize.listeners.ScoreIterationListener - Score at iteration 400 is 0.10802983709162767\n",
      "[pool-4-thread-10] INFO org.deeplearning4j.optimize.listeners.ScoreIterationListener - Score at iteration 500 is 0.06857784103670342\n",
      "[pool-4-thread-10] INFO org.deeplearning4j.optimize.listeners.ScoreIterationListener - Score at iteration 600 is 0.048369936262854886\n",
      "[pool-4-thread-10] INFO org.deeplearning4j.optimize.listeners.ScoreIterationListener - Score at iteration 700 is 0.03671566553726351\n",
      "[pool-4-thread-10] INFO org.deeplearning4j.optimize.listeners.ScoreIterationListener - Score at iteration 800 is 0.029306815264681922\n",
      "[pool-4-thread-10] INFO org.deeplearning4j.optimize.listeners.ScoreIterationListener - Score at iteration 900 is 0.0242212715112556\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Examples labeled as 0 classified by model as 0: 10 times\n",
      "Examples labeled as 1 classified by model as 1: 15 times\n",
      "Examples labeled as 2 classified by model as 1: 7 times\n",
      "Examples labeled as 2 classified by model as 2: 21 times\n",
      "\n",
      "\n",
      "==========================Scores========================================\n",
      " Accuracy:  0.8679\n",
      " Precision: 0.8939\n",
      " Recall:    0.9167\n",
      " F1 Score:  0.9052\n",
      "========================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mnetwork\u001b[0m: INSTANCE.$ref$DeepLearning4JMultiLayerNetwork = cmd3$$user$DeepLearning4JMultiLayerNetwork@4a3ab4e6"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "val network = new DeepLearning4JMultiLayerNetwork(\"data/iris.txt\");\n",
    "network.execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[36mcom.quantifind.charts.highcharts.SeriesType\u001b[0m\n",
       "\u001b[32mimport \u001b[36mcom.quantifind.charts.Highcharts._\u001b[0m\n",
       "\u001b[32mimport \u001b[36mcom.quantifind.charts.highcharts.Highchart\u001b[0m\n",
       "\u001b[32mimport \u001b[36mcom.quantifind.charts.highcharts.Histogram\u001b[0m\n",
       "defined \u001b[32mfunction \u001b[36mshow\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import com.quantifind.charts.highcharts.SeriesType\n",
    "import com.quantifind.charts.Highcharts._\n",
    "import com.quantifind.charts.highcharts.Highchart\n",
    "import com.quantifind.charts.highcharts.Histogram\n",
    "\n",
    "def show(c:Highchart, name:String) = {\n",
    "    val json = s\"\"\"\n",
    "$$(function () {\n",
    "    $$('#$name').highcharts(${c.toJson})\n",
    "    })\n",
    "\"\"\"\n",
    "    display.html(<div id={name}>graph</div>)\n",
    "    display.js(json)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "  <script src=\"http://code.highcharts.com/stock/highstock.js\"></script>\n",
       "  <script src=\"http://code.highcharts.com/stock/modules/exporting.js\"></script>\n",
       "  <script src=\"http://www.highcharts.com/js/themes/grid.js\"></script>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display.html(\n",
    "<div>\n",
    "  <script src=\"http://code.highcharts.com/stock/highstock.js\"></script>\n",
    "  <script src=\"http://code.highcharts.com/stock/modules/exporting.js\"></script>\n",
    "  <script src=\"http://www.highcharts.com/js/themes/grid.js\"></script>\n",
    "</div>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "serving resources from: file:/opt/app/src/main/ipynb/index-1477856146070.html\n",
      "Server started: http://a0973adcac54:41990/index-1477856146070.html\n",
      "Error while opening window (cause: java.io.IOException: Cannot run program \"xdg-open\": error=2, No such file or directory)\n",
      "You can browse the following URL: http://a0973adcac54:41990\n",
      "Output written to http://a0973adcac54:41990 (CMD + Click link in Mac OSX).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div id=\"classifications\">graph</div>"
      ]
     },
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {},
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[36mjava.util\u001b[0m\n",
       "\u001b[32mimport \u001b[36mcom.quantifind.charts.Highcharts._\u001b[0m\n",
       "\u001b[32mimport \u001b[36mcollection.mutable._\u001b[0m\n",
       "\u001b[32mimport \u001b[36mscala.collection.JavaConversions._\u001b[0m\n",
       "\u001b[32mimport \u001b[36mscala.collection.mutable\u001b[0m\n",
       "\u001b[36mlistmax\u001b[0m: List[Int] = \u001b[33mList\u001b[0m(\n",
       "  \u001b[32m2\u001b[0m,\n",
       "  \u001b[32m0\u001b[0m,\n",
       "  \u001b[32m2\u001b[0m,\n",
       "  \u001b[32m2\u001b[0m,\n",
       "  \u001b[32m1\u001b[0m,\n",
       "  \u001b[32m1\u001b[0m,\n",
       "  \u001b[32m2\u001b[0m,\n",
       "  \u001b[32m1\u001b[0m,\n",
       "  \u001b[32m1\u001b[0m,\n",
       "  \u001b[32m1\u001b[0m,\n",
       "  \u001b[32m2\u001b[0m,\n",
       "  \u001b[32m0\u001b[0m,\n",
       "  \u001b[32m1\u001b[0m,\n",
       "  \u001b[32m0\u001b[0m,\n",
       "  \u001b[32m1\u001b[0m,\n",
       "  \u001b[32m2\u001b[0m,\n",
       "  \u001b[32m0\u001b[0m,\n",
       "  \u001b[32m2\u001b[0m,\n",
       "  \u001b[32m2\u001b[0m,\n",
       "\u001b[33m...\u001b[0m\n",
       "\u001b[36mlistrows\u001b[0m: List[Int] = \u001b[33mList\u001b[0m(\n",
       "  \u001b[32m52\u001b[0m,\n",
       "  \u001b[32m48\u001b[0m,\n",
       "  \u001b[32m49\u001b[0m,\n",
       "  \u001b[32m50\u001b[0m,\n",
       "  \u001b[32m51\u001b[0m,\n",
       "  \u001b[32m45\u001b[0m,\n",
       "  \u001b[32m44\u001b[0m,\n",
       "  \u001b[32m47\u001b[0m,\n",
       "  \u001b[32m46\u001b[0m,\n",
       "  \u001b[32m41\u001b[0m,\n",
       "  \u001b[32m40\u001b[0m,\n",
       "  \u001b[32m43\u001b[0m,\n",
       "  \u001b[32m42\u001b[0m,\n",
       "  \u001b[32m37\u001b[0m,\n",
       "  \u001b[32m36\u001b[0m,\n",
       "  \u001b[32m39\u001b[0m,\n",
       "  \u001b[32m38\u001b[0m,\n",
       "  \u001b[32m33\u001b[0m,\n",
       "  \u001b[32m32\u001b[0m,\n",
       "\u001b[33m...\u001b[0m\n",
       "\u001b[36msetrows\u001b[0m: collection.mutable.Set[Integer] = \u001b[33mSet\u001b[0m(\n",
       "  0,\n",
       "  1,\n",
       "  2,\n",
       "  3,\n",
       "  4,\n",
       "  5,\n",
       "  6,\n",
       "  7,\n",
       "  8,\n",
       "  9,\n",
       "  10,\n",
       "  11,\n",
       "  12,\n",
       "  13,\n",
       "  14,\n",
       "  15,\n",
       "  17,\n",
       "  16,\n",
       "  19,\n",
       "\u001b[33m...\u001b[0m\n",
       "\u001b[36mchart\u001b[0m: com.quantifind.charts.highcharts.Highchart = \u001b[33mHighchart\u001b[0m(\n",
       "  \u001b[33mList\u001b[0m(\n",
       "    \u001b[33mSeries\u001b[0m(\n",
       "      \u001b[33mList\u001b[0m(\n",
       "        Data(52,2,None,None),\n",
       "        Data(48,0,None,None),\n",
       "        Data(49,2,None,None),\n",
       "        Data(50,2,None,None),\n",
       "        Data(51,1,None,None),\n",
       "        Data(45,1,None,None),\n",
       "        Data(44,2,None,None),\n",
       "        Data(47,1,None,None),\n",
       "        Data(46,1,None,None),\n",
       "        Data(41,1,None,None),\n",
       "        Data(40,2,None,None),\n",
       "        Data(43,0,None,None),\n",
       "        Data(42,1,None,None),\n",
       "        Data(37,0,None,None),\n",
       "        Data(36,1,None,None),\n",
       "        Data(39,2,None,None),\n",
       "\u001b[33m...\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import java.util\n",
    "import com.quantifind.charts.Highcharts._\n",
    "import collection.mutable._\n",
    "import scala.collection.JavaConversions._\n",
    "import scala.collection.mutable\n",
    "\n",
    "var listmax: List[Int] = List()\n",
    "var listrows: List[Int] = List()\n",
    "val setrows : mutable.Set[Integer] = asScalaSet(network.networkOutput.rowKeySet())\n",
    "for(rowKey <- setrows) {\n",
    "  var rowMap: Map[Integer, Double] = mapAsScalaMap(network.networkOutput.row(rowKey))\n",
    "  var maxValColumn: Int = 0\n",
    "  var maxVal: Double = 0.0\n",
    "  listrows :::= List(rowKey)\n",
    "  for(colKey <- rowMap.keysIterator){\n",
    "    var dval = rowMap.get(colKey).get\n",
    "    if(dval>maxVal){\n",
    "      maxVal = dval\n",
    "      maxValColumn = colKey\n",
    "    }\n",
    "  }\n",
    "\n",
    "  listmax :::= List(maxValColumn)\n",
    "\n",
    "}\n",
    "\n",
    "val chart = scatter(listrows,listmax)\n",
    "show(chart,\"classifications\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala 2.10",
   "language": "scala210",
   "name": "scala210"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala210",
   "pygments_lexer": "scala",
   "version": "2.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}